---
title: "1_data_download"
author: "B Steele"
date: "2023-05-03"
output: 
  html_document
editor_options: 
  chunk_output_type: console
---

This script was adapted from the GROD data download script authored by Matt Ross.

# Downloading data from Google Drive

After people have used Google Earth Engine and our `eePlumB.js` script in the the `1_user_interface` script to identify plumes and blooms, we must download the data and bind it together before checking it for quality. This script does that.

```{r setup}

library(tidyverse)
library(googledrive)
library(lubridate)
library(listviewer)
library(readxl)
library(feather)

tmp_dir = 'eePlumB/2_data_ingestion/data/tmp'
in_dir = 'eePlumB/2_data_ingestion/data/in'
out_dir = 'eePlumB/2_data_ingestion/data/out'
```

Some helper functions

```{r}
driveDown = function(driveid, drivename, dest) {
  drive_download(as_id(driveid), path = file.path(dest, drivename), overwrite = T)
}

findcsv = function(folderid) {
  drive_ls(folderid, pattern = '.csv', recursive = T)
}
```

Here, we load the validation worksheet that has all links for the shared drive folders. I have gone through these manually to remove folders that have had permissions revoked (likely due to someone deleting the folder) and those that have linked only the 'labels' folder. For the most part, these had no data in them, as this was initially used as an example for the eePlumB workflow in an undergrad class.

```{r}
project_folder = drive_ls(pattern = 'Superior Plume-Bloom')
data_file = drive_ls(path = as_id(project_folder$id), recursive = T, pattern = 'Validation Sheet')

driveDown(data_file$id, data_file$name, tmp_dir)

val.sheet <- read_xlsx(file.path(tmp_dir, paste0(data_file$name, '.xlsx')), sheet = 'Volunteers')
```

Here, we will map over the drive link and download the label and validation scripts.

```{r}
for(i in 1:nrow(val.sheet)){
  folders <- drive_ls(as_id(unlist(strsplit(val.sheet$`Drive Folder Link`[i], '\\?'))[1]))
  files = map_dfr(folders$id, findcsv)
  if (nrow(files) > 0) {
    pwalk(list(files$id, paste0(val.sheet$Initials[i], '_', files$name), rep(in_dir, nrow(files))), driveDown)
  }
}
```

Some of the files were not exported correctly, so we need to filter out the files that are 1b (no data).

```{r}
all_files = list.files(in_dir)

sizes = file.info(file.path(in_dir, all_files))$size

okay_files = all_files[which(sizes > 1)]
```

And now, we need to check that the contributor initials match the script initials. For this, we'll set up a helper function.

```{r}
checkInitials = function(filename) {
  init = unlist(str_split(okay_files[22], '_'))[c(1, 3)]
  identical(init[1], init[2])
}

ingest_files = okay_files[unlist(map(okay_files, checkInitials))]
```

Now, let's compile all of these files into a two composite files: a validation dataset and a label dataset.

```{r}
val = ingest_files[grepl('validation', ingest_files)]
label = ingest_files[!grepl('validation', ingest_files)]
```

## Bind that data together and export.

Let's make a helper function to add initials to the labels.

```{r}
addInitials = function(file) {
  init = unlist(str_split(file, '_'))[1]
  df = read.csv(file.path(in_dir, file))
  df$vol_init = init
  df
}
```

Map that helper function and create a dataframe of the validation and labels.

```{r}
all_validation <- map_dfr(val, addInitials)
all_labels <- map_dfr(label,addInitials)

write.csv(all_validation, file.path(out_dir, paste0('collated_validation_data_v', Sys.Date(), '.csv')), row.names = F)
write.csv(all_labels, file.path(out_dir, paste0('collated_label_data_v', Sys.Date(), '.csv')), row.names = F)
```
