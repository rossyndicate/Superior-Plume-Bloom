---
title: "1_data_download"
author: "B Steele"
date: "2023-05-03"
output: 
  html_document
editor_options: 
  chunk_output_type: console
---

This script was adapted from the GROD data download script authored by Matt Ross.

# Downloading data from Google Drive

After people have used Google Earth Engine and our `eePlumB.js` script in the the `1_user_interface` script to identify plumes and blooms, we must download the data and bind it together before checking it for quality. This script does that.

```{r setup}
library(tidyverse)
library(googledrive)
library(lubridate)
library(readxl)

tmp_dir = 'eePlumB/2_data_ingestion/data/tmp'
in_dir = 'eePlumB/2_data_ingestion/data/in'
out_dir = 'eePlumB/2_data_ingestion/data/out'

drive_auth()
```

Some helper functions

```{r}
driveDown = function(driveid, drivename, dest) {
  drive_download(as_id(driveid), path = file.path(dest, drivename), overwrite = T)
}

findcsv = function(folderid) {
  drive_ls(folderid, pattern = '.csv', recursive = T)
}

driveUp = function(dumpid, directory, filename) {
  drive_upload(file.path(directory, filename), filename, overwrite = T, path = as_id(dumpid))
}
```

And then create the needed directory structure to download/save files to and upload files from.

```{r}
dirs = c('eePlumB/2_data_ingestion/data/',
         'eePlumB/2_data_ingestion/data/in', 
         'eePlumB/2_data_ingestion/data/out',
         'eePlumB/2_data_ingestion/data/tmp')
walk(dirs, dir.create)
```

Here, we load the validation worksheet that has all links for the shared drive folders. I have gone through these manually to remove folders that have had permissions revoked (likely due to someone deleting the folder) and those that have linked only the 'labels' folder. For the most part, these had no data in them, as this was initially used as an example for the eePlumB workflow in an undergrad class.

```{r}
project_folder = drive_ls(pattern = 'Superior Plume-Bloom')
data_file = drive_ls(path = as_id(project_folder$id), recursive = T, pattern = 'Validation Sheet')

driveDown(data_file$id, data_file$name, tmp_dir)

val.sheet <- read_xlsx(file.path(tmp_dir, paste0(data_file$name, '.xlsx')), sheet = 'Volunteers')
```

Here, we will map over the drive link and download the label and validation scripts.

```{r}
for(i in 1:nrow(val.sheet)){
  folders <- drive_ls(as_id(unlist(strsplit(val.sheet$`Drive Folder Link`[i], '\\?'))[1]))
  files = map_dfr(folders$id, findcsv)
  if (nrow(files) > 0) {
    pwalk(list(files$id, paste0(val.sheet$Initials[i], '_', files$name), rep(in_dir, nrow(files))), driveDown)
  }
}
```

Some of the files were not exported correctly, so we need to filter out the files that are 1b (no data).

```{r}
all_files = list.files(in_dir)

sizes = file.info(file.path(in_dir, all_files))$size

okay_files = all_files[which(sizes > 1)]
```

And now, we need to check that the contributor initials match the script initials. For this, we'll set up a helper function.

```{r}
checkInitials = function(filename) {
  init = unlist(str_split(filename, '_'))[c(1, 3)]
  identical(init[1], init[2])
}

ingest_files = okay_files[unlist(map(okay_files, checkInitials))]
```

For posterity, let's upload these to the Drive in case folks delete their folders later.

```{r}
#get the folder info
raw_loc = drive_ls(as_id(project_folder$id), pattern = 'eePlumB_raw_data', recursive = T)

pwalk(list(rep(raw_loc$id, length(okay_files)), rep(in_dir, length(okay_files)), okay_files), driveUp)
```

## Prep the files for ingest to GEE

Now, let's compile all of these files into a two composite files: a validation dataset and a label dataset.

```{r}
val = ingest_files[grepl('validation', ingest_files)]
label = ingest_files[!grepl('validation', ingest_files)]
```

Let's make a helper function to add initials to the labels.

```{r}
#for the validation script
addInitials = function(file) {
  df = read.csv(file.path(in_dir, file))
  init = unlist(str_split(file, '_'))[1]
  df$vol_init = init
  df
}
#for the labels
addInitMissDate = function(file) {
  df = read.csv(file.path(in_dir, file))
  init = unlist(str_split(file, '_'))[1]
  miss = unlist(str_split(file, '_'))[4]
  date = unlist(str_split(file, '_'))[5]
  df$vol_init = init
  df$mission = miss
  df$date = date
  df
}
```

Map that helper function and create a dataframe of the validation and labels.

```{r}
all_validation <- map_dfr(val, addInitials)
all_labels <- map_dfr(label,addInitMissDate)
```

## Data harmonization

The label data need some clean up for column names/data types. Just due to the nature of the data, we've got some wonky stuff going on in here.

First, harmonize the RGB column. RGB = B4, 3, 2 for both LS and Sentinel. Looks like it's just Landsat where RGB are missing and instead stored in SR_Bx columns.

```{r}
all_labels_harmonized <- all_labels %>% 
  mutate(Red = if_else(is.na(Red), 
                       SR_B4, 
                       Red),
         Green = if_else(is.na(Green),
                         SR_B3,
                         Green),
         Blue = if_else(is.na(Blue),
                        SR_B2,
                        Blue)) %>% 
  select(-c(SR_B2, SR_B3, SR_B4))
```

And then rename the Landsat SR 5-7 as just B5-7 to match Sentinel data

```{r}
all_labels_harmonized  <- all_labels_harmonized %>%
  mutate(B5 = if_else(is.na(B5),
                      SR_B5,
                      B5),
         B6 = if_else(is.na(B6),
                      SR_B6,
                      B6),
         B7 = if_else(is.na(B7),
                      SR_B7,
                      B7)) %>% 
  select(-c(SR_B5, SR_B6, SR_B7))
```

And now let's re-org some of the column names for future us. 

```{r}
all_labels_harmonized <- all_labels_harmonized %>% 
  select(date, mission, lat, lon, class, Red, Green, Blue, B5, B6, B7, B8, B11, B12, vol_init)
```

Save the .csv to the temp folder

```{r}
write_csv(all_validation, file.path(out_dir, paste0('collated_validation_data_v', Sys.Date(), '.csv')))
write_csv(all_labels_harmonized, file.path(out_dir, paste0('collated_label_data_v', Sys.Date(), '.csv')))

```

And now, upload those files to the Drive

```{r}
#get the folder info
coll_loc = drive_ls(as_id(project_folder$id), pattern = 'eePlumB_collated', recursive = T)

upload_list = list.files(out_dir)

pwalk(list(rep(coll_loc$id, length(upload_list)), rep(out_dir, length(upload_list)), upload_list), driveUp)
```

## Clean up directory

```{r}
unlink("eePlumB/2_data_ingestion/data", recursive= T)
```
