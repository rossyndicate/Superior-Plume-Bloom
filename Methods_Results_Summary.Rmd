---
title: "Methods and Results Summary"
author: "ROSSyndicate"
date: "2023-08-07"
output: html_document
bibliography: references.bib
---

```{r, include = F}
packages <- c('tidyverse', 'gt')

package_loader <- function(x) {
    if (x %in% installed.packages()) {
      library(x, character.only = TRUE)
    } else {
      install.packages(x)
      library(x, character.only = TRUE)
    }
}

lapply(packages, package_loader)
```

# Introduction

Remote sensing image classification is common in terrestrial applications (in particular, land use and land cover), but has not been applied in aquatic environments beyond general presence and absence of water and wetlands. The primary exception to the use of image classification in aquatic environments is assessing the presence of submerged aquatic vegetation ("SAV") (e.g., [@visser2018], [@e.l.hestir2012]); however, these classification methods require high resolution imagery with high spectral resolution often from land-based high-resolution photography or unpersoned aerial vehicles ("UAVs").

In the Great Lakes (GL) region, much of the use of image classification is completed using moderate resolution (e.g., Landsat, Sentinel, MODIS) satellite images, focusing on mapping the distribution and types of wetlands throughout the region ([@mohseni2023], [@v.l.valenti2020]), as well as SAV distribution throughout the system ([@wolter2005]). Most of these analyses focus on a relatively short temporal period (months to years), while a some span the entire Landsat archive from the mid '80s through the recent past (e.g., [@amani2022]).

In the recent past, much attention has been paid to the apparent proliferation of algal blooms in some of the clearest lakes, including Lake Superior (cite). While detecting algal blooms from moderate-resolution satellite imagery is difficult due to low temporal frequency, time of day of acquisition, pixel size, and spectral band metrics (cite), as well as the lack of observed, spatially-explicit bloom observations to validate presence and absence, detecting sediment plumes (which often precede algal blooms) is relatively easy with just the red, green, and blue bands common on nearly all moderate-resolution satellites.

In this analysis, we use the Landsat Collection 2 Surface Reflectance product archive (missions 5 through 9) and the Sentinel 2 Surface Reflectance product archive, a novel crowd-sourced label data set (eePlumB), and Google Earth Engine to create image classification models to create a time series of rasters that enumerate sediment plumes across the western arm of Lake Superior.

[[image of AOI]]

# Methods

## eePlumB

Using the overarching architecture presented in the Global Rivers Obstruction Database (GROD) ([@yang2022]) to engage volunteer observers, we croudsourced class labels for Landsat and Sentinel-2 images for the following classes: 'cloud', 'open water', 'light near shore sediment', 'dark near shore sediment', 'offshore sediment', 'shoreline contamination', 'other', and 'algae bloom' using our Earth Engine Plume and Bloom labeling interface ("eePlumB").

[[more here at a later date, borrow from the eePlumB documentation already created]]

In order to eliminate outlier band information and reduce noise in the input for our models, the second and ninety-eighth percentiles were calculated for each mission-band combination and label data associated with values outside of those cutoffs were dropped from the analysis. [[Could add the `02_label_class_summaries.Rmd` as supplemental.]]

## Model development

We used the built-in gradient tree boost ("GTB") ee.Classifier() method within Google Earth Engine to create classification models from the crowd-sourced label data. Label data were randomly split into training (70%) and test (30%) data sets, with no special handling procedures for classes or satellite missions. Data were examined to assure that all classes and missions were present in both the training and testing data sets.

GTB models for each mission were trained independently on the rescaled band data from red, green, blue, near infrared, and both shortwave infrared bands for Landsat missions to classify 5 categories: cloud, open water, light near shore sediment, dark near shore sediment, and offshore sediment. For Sentinel-2, the bands used to develop the classifier were red, green, blue, red edge 1-3, near infrared, and both short-wave infrared bands. We did not tune the hyperparameters for the GTB model, as performance was already acceptable for discerning open water from sediment plume using 10 trees.

[[if we develop this into a MS, we may wish to add more here, including hyperparameter tuning and/or use of other ee.Classifiers). Most papers of this nature just use all 4 - like we did with the Yugo model - with iterations for hypertuning.]]

## Image classification

### Image Pre-processing

Mosaic-ed images were made for each mission-date as mean band values where any two path row or tiles overlapped. All Landsat mission-date images were pre-processed to remove saturated pixels, and only images with an image quality greater than or equal to 7 were included. Sentinel-2 bands that had a pixel resoluiton greater than 10m x 10m were reprojected (downsampled) to 10m x 10m pixel sizes so that the GTB model could be applied to the composite images more efficiently. No further pre-processing was completed on the Sentinel-2 data.

Three areas of interest (AOIs) were used in this analysis: the complete AOI, the AOI without shoreline contamination, and the AOI with shoreline contamination. The area of shoreline contamination was defined as any area within 60 meters of a volunteer-identified pixel with shoreline contamination. We assumed that shoreline contamination was consistent throughout the analysis and was not specific to any particular satellite or time period.

### Model application and summaries

Each GTB model was applied to the corresponding satellite image stack and two data types were output: a tabular data summary of the area classified and the total area of each class for all three AOIs, as well as a .tif raster at the resolution the GTB was applied (10m for Sentinel-2 and 30m for Landsat) for each classified mission-date image.

## Model evaluation metrics

Models were evaluated through confusion matrices, error matrices, kappa statistics, F1 statistics for each class, and weighted F1 statistics per model.

-   confusion matrix - training: an indication per label class where the model-assigned class and label class are compared.
-   error matrix - testing: given the test data, does the model assign the correct class? These are tibble-style summaries where the model-assigned class and label class are compared.
-   *overall accuracy\*\*: percentage of model-assigned labels that match the test dataset, an indicator of model precision. No weights are given to any class based on the number of labels.*
-   *training accuracy\*\*: how well does the classification model assign the correct class to the training data, an indicator of model precision.*
-   kappa statistic: an indicator of how much better or worse a model performs than by random chance. score is -1 to 1, where 0 is the same as random chance, positive values are better than random chance and negative are poorer than random chance
-   F1 score: the harmonic mean of precision and recall per class (beta = 1, hence F1 where precision and recall are evenly weighted)
-   weighted F1: class-weighted (by number of labels) F1 score across all classes (sum of the proportional F1 score per class)

*\*\*We have unequal distribution of classes, so should not use overall accuracy metrics as they can be misleading when the distribution of classes are not equal.*

Models were evaluated as 5-class categories and 3-class categories where all sediment categories were compiled into a single class.

# Results

## Label dataset

```{r, include = F, echo = F}
ml_labels = c('cloud', 'openWater', 'darkNearShoreSediment', 'lightNearShoreSediment', 'offShoreSediment')
# load labels
labels <- read_csv('data/labels/collated_label_data_v2023-07-20.csv')
only_ml_labels <- labels %>%
  filter(class %in% ml_labels)
filtered_labels <- read_csv('data/labels/collated_label_data_filtered_v2023-07-20.csv')
label_table <- labels %>%
  group_by(mission, class) %>%
  tally() %>%
  pivot_wider(names_from = mission, values_from = n) %>%
  rename('Landsat 5 (all)' = 'LS5',
         'Landsat 7 (all)' = 'LS7',
         'Landsat 8 (all)' = 'LS8',
         'Landsat 9 (all)' = 'LS9',
         'Sentinel-2 (all)' = 'SEN2')

filtered_label_table <- filtered_labels %>%
  group_by(mission, class) %>%
  tally() %>%
  pivot_wider(names_from = mission, values_from = n) %>%
  rename('Landsat 5 (filtered)' = 'LS5',
         'Landsat 7 (filtered)' = 'LS7',
         'Landsat 8 (filtered)' = 'LS8',
         'Landsat 9 (filtered)' = 'LS9',
         'Sentinel-2 (filtered)' = 'SEN2')

label_table_join <- full_join(label_table, filtered_label_table)
```

The collated crowdsourced label dataset consisted of `r nrow(labels)` labels for across all classes. There were `r nrow(ml_labels)` labels that were part of the classes of interest (cloud, open water, sediment). After filtering for outliers from each subset of mission-specific labels, there were `r nrow(filtered_labels)` labels with complete band information. Table 1 presents a break down of the labels.

```{r, echo = F}
gt(label_table_join) %>%
  cols_align(., 'center', columns = c('Landsat 5 (all)':'Sentinel-2 (filtered)')) %>%
  tab_spanner('all labels',
              c('Landsat 5 (all)':'Sentinel-2 (all)')) %>%
  tab_spanner('filtered for model development and testing',
              c('Landsat 5 (filtered)':'Sentinel-2 (filtered)')) %>%
  tab_style(cell_text(size = 'small'), cells_body()) %>%
  tab_style(cell_text(size = 'small'), cells_column_labels()) %>%
  tab_footnote('Table 1. Summary of eePlumB classes per mission (version 2023-07-20). All labels (left group) are the raw number of collated labels, the right group are labels that have been filtered as described.')
```

```{r, include = F, echo = F}
mission_date_summary <- labels %>%
  group_by(mission, date) %>%
  tally()
mission_date_summary_filtered <- filtered_labels %>%
  group_by(mission, date) %>%
  tally()
md_summ_table <- mission_date_summary %>%
  group_by(mission) %>%
  tally() %>%
  rename('unique mission-dates (all)' = 'n')
md_summ_table_filt <- mission_date_summary_filtered  %>%
  group_by(mission) %>%
  tally() %>%
  rename('unique mission-dates (filtered)' = 'n')

summary_table_join <- full_join(md_summ_table, md_summ_table_filt)
```

Labels were present from `r nrow(mission_date_summary)` individual mission-date combinations spanning the dates of `r min(mission_date_summary$date)` to `r max(mission_date_summary$date)`. Labels in the filtered dataset were present from `r nrow(mission_date_summary_filtered)` mission-date combinations spanning the dates `r min(mission_date_summary_filtered$date)` to `r max(mission_date_summary_filtered$date)`. See Table 2 for a complete breakdown of labels by mission-date combination.

```{r, echo = F}
gt(summary_table_join) %>%
  cols_align(., 'center', columns = c('unique mission-dates (all)', 'unique mission-dates (filtered)')) %>%
  tab_style(cell_text(size = 'small'), cells_body()) %>%
  tab_style(cell_text(size = 'small'), cells_column_labels()) %>%
  tab_footnote('Table 2. Summary of eePlumB mission-dates present in the label dataset. When filtered for ouliers, only two mission-dates, both Landsat 5 images, were lost from the dataset.')
```

## Model evaluation

Models performance was acceptable across open water, cloud, and the generalized sediment (where the 3 sediment categories were grouped together).

## Model application

look at n images over time (histo-ts distribution)

look at proportion of aoi classified over time?

# References

```{r, echo = F}
knitr::wrap_rmd('Methods_Results_Summary.Rmd', width = 80, backup = NULL)
```
