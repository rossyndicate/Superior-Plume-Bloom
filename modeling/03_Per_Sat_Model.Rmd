---
title: "eePlumB per-Satellite Model"
author: "ROSSyndicate"
date: "2023-08-03"
output: html_document
editor_options:
  markdown:
    wrap: 80
---

```{r setup, echo = F}
libs = c('reticulate', 'tidyverse')

package_loader <- function(x) {
    if (x %in% installed.packages()) {
      library(x, character.only = TRUE)
    } else {
      install.packages(x)
      library(x, character.only = TRUE)
    }
}

lapply(libs, package_loader)
```

# Purpose

This script develops and applies Gradient Tree Boost Models to Landsat and
Sentinel mosaic-ed images.

## Activate conda environment

Check for virtual environment and activate, otherwise, set up virtual
environment.

```{r, conda env}
if (!dir.exists("env")) {
  source("pySetup.R")
} else {
  use_condaenv(file.path(getwd(), "env"))
}
```

And then import the needed modules

```{python}
import ee
import os
import time
import pandas as pd
import geopandas as gpd
import geemap as gm

v_date = '2023-08-03'
```

## GEE Setup

In order to access the GEE API, we need to authenticate our identity. To do
this, you must type `earthengine authenticate` in the terminal window. When your
browser states 'You are now authenticated with the gcloud CLI', the
authentication is complete. This authentication is valid for 7 days.

Now, we need to initialize our GEE session.

```{python}
ee.Initialize(project = 'ee-ross-superior')
```

# Import assets

## Load the labels file into the environment

Because of how the later function is written, it works best to read these into
R. I couldn't tell you why other than my function doesn't work with pandas
dataframes.

```{r}
labels_file <- list.files('data/labels/', full.names = T)
labels <- read_csv(labels_file)

# subset for sen/ls
sen_labels = labels %>% 
  filter(mission == 'SEN2')
ls_labels = labels %>% 
  filter(mission != 'SEN2')

# and for 5/7/8/9
ls5_labels = labels %>% 
  filter(mission == 'LS5')
ls7_labels = labels %>% 
  filter(mission == 'LS7')
ls8_labels = labels %>% 
  filter(mission == 'LS8')
ls9_labels = labels %>% 
  filter(mission == 'LS9')
```

### Quick QAQC of the labels

We know there are some outliers in here. To create a good model, we'll want to
remove any 'outliers' from each of the label sets. Because we're creating models
for each mission, we'll perform these outlier clean ups for each mission. We'll
define 'outliers' as the outer 0.02 and 0.98 percentiles.

Make some helper lists

```{r}
class_list = c("cloud", "openWater", "lightNearShoreSediment",
               "darkNearShoreSediment", "offShoreSediment")

ls_band_list = c(expr(Red), expr(Green), expr(Blue), expr(B5), expr(B6), expr(B7))
sen_band_list = c(expr(Red), expr(Green), expr(Blue), expr(B5), expr(B6), expr(B7), expr(B8), expr(B11), expr(B12))
```

We need to rescale B11 and B12 for the Sentinel data. While this will not change
any downstream 'outlier' handling, it will [hopefully] help with the wonky
application of Sentinel models since the range is so different for those bands.

```{r}
sen_labels <- sen_labels %>% 
  mutate_at(vars(B11, B12),
            ~ .*0.0001)
```

#### Landsat 5

Maybe a tidyfail, maybe a me problem, but we're creating new, very similar
functions for each dataset.

```{r}
qaqc_filter_ls5 <- function(band, cl) {
  df <- ls5_labels %>% filter(class == cl)
  values = df[{{ band }}] %>% as.vector(.)
  df_sub = df %>% select(date, mission, lat, lon, class, vol_init, {{ band }})
  cuts = quantile(values[[1]], c(0.02, 0.98), na.rm = T)
  df_sub %>% 
    filter(cuts[1] < {{ band }}, {{ band }} < cuts[2])
}

apply_qaqc_bands_ls5 <- function(bn) {
  map2_dfr(c(bn, bn, bn, bn, bn),
           class_list,
           qaqc_filter_ls5)
}
ls_5_filtered <-  map(ls_band_list, apply_qaqc_bands_ls5)

ls_5_filt <- ls_5_filtered %>% 
  reduce(full_join) %>%
  distinct()

```

#### Landsat 7

```{r}
qaqc_filter_ls7 <- function(band, cl) {
  df <- ls7_labels %>% filter(class == cl)
  values = df[{{ band }}] %>% as.vector(.)
  df_sub = df %>% select(date, mission, lat, lon, class, vol_init, {{ band }})
  cuts = quantile(values[[1]], c(0.02, 0.98), na.rm = T)
  df_sub %>% 
    filter(cuts[1] < {{ band }}, {{ band }} < cuts[2])
}

apply_qaqc_bands_ls7 <- function(bn) {
  map2_dfr(c(bn, bn, bn, bn, bn),
           class_list,
           qaqc_filter_ls7)
}
ls_7_filtered <-  map(ls_band_list, apply_qaqc_bands_ls7)

ls_7_filt <- ls_7_filtered %>% 
  reduce(full_join) %>% 
  distinct()
```

#### Landsat 8

```{r}
qaqc_filter_ls8 <- function(band, cl) {
  df <- ls8_labels %>% filter(class == cl)
  values = df[{{ band }}] %>% as.vector(.)
  df_sub = df %>% select(date, mission, lat, lon, class, vol_init, {{ band }})
  cuts = quantile(values[[1]], c(0.02, 0.98), na.rm = T)
  df_sub %>% 
    filter(cuts[1] < {{ band }}, {{ band }} < cuts[2])
}

apply_qaqc_bands_ls8 <- function(bn) {
  map2_dfr(c(bn, bn, bn, bn, bn),
           class_list,
           qaqc_filter_ls8)
}

ls_8_filtered <-  map(ls_band_list, apply_qaqc_bands_ls8)

ls_8_filt <- ls_8_filtered %>% 
  reduce(full_join) %>% 
  distinct()

```

#### Landsat 9

```{r}
qaqc_filter_ls9 <- function(band, cl) {
  df <- ls9_labels %>% filter(class == cl)
  values = df[{{ band }}] %>% as.vector(.)
  df_sub = df %>% select(date, mission, lat, lon, class, vol_init, {{ band }})
  cuts = quantile(values[[1]], c(0.02, 0.98), na.rm = T)
  df_sub %>% 
    filter(cuts[1] < {{ band }}, {{ band }} < cuts[2])
}

apply_qaqc_bands_ls9 <- function(bn) {
  map2_dfr(c(bn, bn, bn, bn, bn),
           class_list,
           qaqc_filter_ls9)
}

ls_9_filtered <-  map(ls_band_list, apply_qaqc_bands_ls9)

ls_9_filt <- ls_9_filtered %>% 
  reduce(full_join) %>% 
  distinct()

```

#### Sentinel 2

```{r}
qaqc_filter_sen <- function(band, cl) {
  df <- sen_labels %>% filter(class == cl)
  values = df[{{ band }}] %>% as.vector(.)
  df_sub = df %>% select(date, mission, lat, lon, class, vol_init, {{ band }})
  cuts = quantile(values[[1]], c(0.02, 0.98), na.rm = T)
  df_sub %>% 
    filter(cuts[1] < {{ band }}, {{ band }} < cuts[2])
}

apply_qaqc_bands_sen <- function(bn) {
  map2_dfr(c(bn, bn, bn, bn, bn),
           class_list,
           qaqc_filter_sen)
}

sen_filtered <- map(sen_band_list, apply_qaqc_bands_sen)

sen_filt <- sen_filtered %>% 
  reduce(full_join) %>% 
  distinct()

```

#### Check for complete obs

```{r}
ls_bands_text = c('Red', 'Green', 'Blue', 'B5', 'B6', 'B7')
sen_bands_text = c('Red', 'Green', 'Blue', 'B5', 'B6', 'B7', 'B8', 'B11', 'B12')

ls_5_filt <- ls_5_filt %>% 
  filter_at(vars(all_of(ls_bands_text)), all_vars(!is.na(.)))
ls_7_filt <- ls_7_filt %>% 
  filter_at(vars(all_of(ls_bands_text)), all_vars(!is.na(.)))
ls_8_filt <- ls_8_filt %>% 
  filter_at(vars(all_of(ls_bands_text)), all_vars(!is.na(.)))
ls_9_filt <- ls_9_filt %>% 
  filter_at(vars(all_of(ls_bands_text)), all_vars(!is.na(.)))
sen_filt <- sen_filt %>% 
  filter_at(vars(all_of(sen_bands_text)), all_vars(!is.na(.)))
```

#### Make ee feature collections

Transform each of the r dataframes to ee feature collections

```{python}
def ls_to_eeFeat(df):
  features=[]
  for i in range(df.shape[0]):
    x,y = df.lon[i],df.lat[i]
    latlong =[x,y]
    loc_properties = ({'class': str(df['class'][i]), # note 'class' is a special word, must use different syntax
      'mission': str(df.mission[i]), 
      'Red': df.Red[i],
      'Green': df.Green[i],
      'Blue': df.Blue[i],
      'B5': df.B5[i],
      'B6': df.B6[i],
      'B7': df.B7[i]
      })
    g = ee.Geometry.Point(latlong, 'EPSG:4326')
    feature = ee.Feature(g, loc_properties)
    features.append(feature)
  ee_object = ee.FeatureCollection(features)
  return ee_object

ee_ls5_labels = ls_to_eeFeat(r.ls_5_filt)
ee_ls7_labels = ls_to_eeFeat(r.ls_7_filt)
ee_ls8_labels = ls_to_eeFeat(r.ls_8_filt)
ee_ls9_labels = ls_to_eeFeat(r.ls_9_filt)

def sen_to_eeFeat(df):
  features=[]
  for i in range(df.shape[0]):
    x,y = df.lon[i],df.lat[i]
    latlong =[x,y]
    loc_properties = ({'class': str(df['class'][i]), # note 'class' is a special word, must use different syntax
      'mission': str(df.mission[i]), 
      'Red': df.Red[i],
      'Green': df.Green[i],
      'Blue': df.Blue[i],
      'B5': df.B5[i],
      'B6': df.B6[i],
      'B7': df.B7[i],
      'B8': df.B8[i],
      'B11': df.B11[i],
      'B12': df.B12[i]
      })
    g = ee.Geometry.Point(latlong, 'EPSG:4326')
    feature = ee.Feature(g, loc_properties)
    features.append(feature)
  ee_object = ee.FeatureCollection(features)
  return ee_object

ee_sen_labels = sen_to_eeFeat(r.sen_filt)
```

Filter the label sets for the classes we're interested in labeling

```{python}
# point to class values we're interested in labeling
class_values = (['cloud',
  'openWater',
  'lightNearShoreSediment',
  'offShoreSediment',
  'darkNearShoreSediment'])

ee_ls5_labels_filt = ee_ls5_labels.filter(ee.Filter.inList('class', class_values))
ee_ls7_labels_filt = ee_ls7_labels.filter(ee.Filter.inList('class', class_values))
ee_ls8_labels_filt = ee_ls8_labels.filter(ee.Filter.inList('class', class_values))
ee_ls9_labels_filt = ee_ls9_labels.filter(ee.Filter.inList('class', class_values))

ee_sen_labels_filt = ee_sen_labels.filter(ee.Filter.inList('class', class_values))
```

## Create Gradient Boost Tree Model

Define the input features and output labels

```{python}
ls_input_feat = ["Red", "Green", "Blue", "B5", "B6", "B7"]
sen_input_feat = ["Red", "Green", "Blue", "B5", "B6", "B7", 'B8', 'B11', 'B12']
output_label = "class"
```

Remap the label values to a 0-based sequential series.

```{python}
remap_values = ee.List.sequence(0, 4)
labels_ls5_map = ee_ls5_labels_filt.remap(ee.List(class_values), remap_values, ee.String(output_label))
labels_ls7_map = ee_ls7_labels_filt.remap(ee.List(class_values), remap_values, ee.String(output_label))
labels_ls8_map = ee_ls8_labels_filt.remap(ee.List(class_values), remap_values, ee.String(output_label))
labels_ls9_map = ee_ls9_labels_filt.remap(ee.List(class_values), remap_values, ee.String(output_label))
labels_sen_map = ee_sen_labels_filt.remap(ee.List(class_values), remap_values, ee.String(output_label))

def class_to_byte(feature):
  byte_value = ee.Number(feature.get(output_label)).toByte()
  return feature.set('byte_property', byte_value)

labels_ls5_map = labels_ls5_map.map(class_to_byte)
labels_ls7_map = labels_ls7_map.map(class_to_byte)
labels_ls8_map = labels_ls8_map.map(class_to_byte)
labels_ls9_map = labels_ls9_map.map(class_to_byte)
labels_sen_map = labels_sen_map.map(class_to_byte)

def add_class_by_remap(feature):
  class_no = ee.Number(feature.get(output_label))
  return feature.set('class', ee.List(class_values).get(class_no))

labels_ls5_map = labels_ls5_map.map(add_class_by_remap)
labels_ls7_map = labels_ls7_map.map(add_class_by_remap)
labels_ls8_map = labels_ls8_map.map(add_class_by_remap)
labels_ls9_map = labels_ls9_map.map(add_class_by_remap)
labels_sen_map = labels_sen_map.map(add_class_by_remap)
```

## Split labels into train/test sets

```{python}
split = 0.7 # percentage of data to use for training
labels_ls5_map = labels_ls5_map.randomColumn('random') #set up a random column
labels_ls7_map = labels_ls7_map.randomColumn('random') #set up a random column
labels_ls8_map = labels_ls8_map.randomColumn('random') #set up a random column
labels_ls9_map = labels_ls9_map.randomColumn('random') #set up a random column
labels_sen_map = labels_sen_map.randomColumn('random') #set up a random column

training_ls5 = labels_ls5_map.filter(ee.Filter.lt("random", split))
training_ls7 = labels_ls7_map.filter(ee.Filter.lt("random", split))
training_ls8 = labels_ls8_map.filter(ee.Filter.lt("random", split))
training_ls9 = labels_ls9_map.filter(ee.Filter.lt("random", split))
training_sen = labels_sen_map.filter(ee.Filter.lt("random", split))
print('Training:')
print(training_ls5.aggregate_histogram('class').getInfo())
print(training_ls7.aggregate_histogram('class').getInfo())
print(training_ls8.aggregate_histogram('class').getInfo())
print(training_ls9.aggregate_histogram('class').getInfo())
print(training_sen.aggregate_histogram('class').getInfo())

testing_ls5 = labels_ls5_map.filter(ee.Filter.gte("random", split))
testing_ls7 = labels_ls7_map.filter(ee.Filter.gte("random", split))
testing_ls8 = labels_ls8_map.filter(ee.Filter.gte("random", split))
testing_ls9 = labels_ls9_map.filter(ee.Filter.gte("random", split))
testing_sen = labels_sen_map.filter(ee.Filter.gte("random", split))
print('Testing:')
print(testing_ls5.aggregate_histogram('class').getInfo())
print(testing_ls7.aggregate_histogram('class').getInfo())
print(testing_ls8.aggregate_histogram('class').getInfo())
print(testing_ls9.aggregate_histogram('class').getInfo())
print(testing_sen.aggregate_histogram('class').getInfo())
```

## Train the GTB model

### Landsat 5

```{python}
trainedGTB_ls5 = (ee.Classifier.smileGradientTreeBoost(10).train(
  features = training_ls5,
  classProperty = 'byte_property',
  inputProperties = ls_input_feat
))
```

### Landsat 7

```{python}
trainedGTB_ls7 = (ee.Classifier.smileGradientTreeBoost(10).train(
  features = training_ls7,
  classProperty = 'byte_property',
  inputProperties = ls_input_feat
))
```

### Landsat 8

```{python}
trainedGTB_ls8 = (ee.Classifier.smileGradientTreeBoost(10).train(
  features = training_ls8,
  classProperty = 'byte_property',
  inputProperties = ls_input_feat
))
```

### Landsat 9

```{python}
trainedGTB_ls9 = (ee.Classifier.smileGradientTreeBoost(10).train(
  features = training_ls9,
  classProperty = 'byte_property',
  inputProperties = ls_input_feat
))
```

### Sentinel 2

```{python}
trainedGTB_sen = (ee.Classifier.smileGradientTreeBoost(10).train(
  features = training_sen,
  classProperty = 'byte_property',
  inputProperties = sen_input_feat
))
```

## Evaluate the models

### Landsat 5

```{python}
trainingAcuracyGTB_ls5 = trainedGTB_ls5
  .confusionMatrix()

confusionMatrixGTB_ls5 = (testing_ls5
  .classify(trainedGTB_ls5)
  .errorMatrix('byte_property', "classification"))
#convert to pandas dataframe with class info
confusion_l5 = (pd.DataFrame(
  confusionMatrixGTB_ls5.getInfo(),
  index=[class_values],
  columns =[class_values]
  ))
print('GTB Confusion Matrix for Landsat 5:')
print(confusion_l5)

#reformat and save
confusion_l5['mission'] = 'Landsat 5'
confusion_l5.reset_index(inplace = True)
confusion_l5 = confusion_l5.rename(columns = {'level_0': 'class'})  
confusion_l5.to_csv('data/output/GTB_'+v_date+'_L5_confusion.csv', index = False)

acc_values_GTB_ls5 = (confusionMatrixGTB_ls5.accuracy().getInfo())
print("GTB Confusion Overall Accuracy for Landsat 5: ", acc_values_GTB_ls5)
k_GTB_ls5 = (confusionMatrixGTB_ls5.kappa().getInfo())
print("GTB kappa for LS5: ", k_GTB_ls5)
fs_GTB_ls5 = (confusionMatrixGTB_ls5.fscore().getInfo())
print('GTB fScore for each class: ', fs_GTB_ls5)
```

### Landsat 7

```{python}
confusionMatrixGTB_ls7 = (testing_ls7
  .classify(trainedGTB_ls7)
  .errorMatrix('byte_property', "classification"))
#convert to pandas dataframe with class info
confusion_l7 = (pd.DataFrame(
  confusionMatrixGTB_ls7.getInfo(),
  index=[class_values],
  columns =[class_values]
  ))
print('GTB Confusion Matrix for Landsat 7:')
print(confusion_l7)

#reformat and save
confusion_l7['mission'] = 'Landsat 7'
confusion_l7.reset_index(inplace = True)
confusion_l7 = confusion_l7.rename(columns = {'level_0': 'class'})  
confusion_l7.to_csv('data/output/GTB_'+v_date+'_L7_confusion.csv', index = False)

acc_values_GTB_ls7 = (confusionMatrixGTB_ls7.accuracy().getInfo())
print("GTB Confusion Overall Accuracy for Landsat 7: ", acc_values_GTB_ls7)
k_GTB_ls7 = (confusionMatrixGTB_ls7.kappa().getInfo())
print("GTB kappa for LS7: ", k_GTB_ls7)
fs_GTB_ls7 = (confusionMatrixGTB_ls7.fscore().getInfo())
print('GTB fScore for each class: ', fs_GTB_ls7)
```

### Landsat 8

```{python}
confusionMatrixGTB_ls8 = (testing_ls8
  .classify(trainedGTB_ls8)
  .errorMatrix('byte_property', "classification"))
#convert to pandas dataframe with class info
confusion_l8 = (pd.DataFrame(
  confusionMatrixGTB_ls8.getInfo(),
  index=[class_values],
  columns =[class_values]
  ))
print('GTB Confusion Matrix for Landsat 8:')
print(confusion_l8)

#reformat and save
confusion_l8['mission'] = 'Landsat 8'
confusion_l8.reset_index(inplace = True)
confusion_l8 = confusion_l8.rename(columns = {'level_0': 'class'})  
confusion_l8.to_csv('data/output/GTB_'+v_date+'_L8_confusion.csv', index = False)
  
acc_values_GTB_ls8 = (confusionMatrixGTB_ls8.accuracy().getInfo())
print("GTB Confusion Overall Accuracy for Landsat 8: ", acc_values_GTB_ls8)
k_GTB_ls8 = (confusionMatrixGTB_ls8.kappa().getInfo())
print("GTB kappa for LS8: ", k_GTB_ls8)
fs_GTB_ls8 = (confusionMatrixGTB_ls8.fscore().getInfo())
print('GTB fScore for each class: ', fs_GTB_ls8)
```

### Landsat 9

```{python}
confusionMatrixGTB_ls9 = (testing_ls9
  .classify(trainedGTB_ls9)
  .errorMatrix('byte_property', "classification"))
#convert to pandas dataframe with class info
confusion_l9 = (pd.DataFrame(
  confusionMatrixGTB_ls9.getInfo(),
  index=[class_values],
  columns =[class_values]
  ))
print('GTB Confusion Matrix for Landsat 9:')
print(confusion_l9)

#reformat and save
confusion_l9['mission'] = 'Landsat 9'
confusion_l9.reset_index(inplace = True)
confusion_l9 = confusion_l9.rename(columns = {'level_0': 'class'})  
confusion_l9.to_csv('data/output/GTB_'+v_date+'_L9_confusion.csv', index = False)

acc_values_GTB_ls9 = (confusionMatrixGTB_ls9.accuracy().getInfo())
print("GTB Confusion Overall Accuracy for Landsat 9: ", acc_values_GTB_ls9)
k_GTB_ls9 = (confusionMatrixGTB_ls9.kappa().getInfo())
print("GTB kappa for LS9: ", k_GTB_ls9)
fs_GTB_ls9 = (confusionMatrixGTB_ls9.fscore().getInfo())
print('GTB fScore for each class: ', fs_GTB_ls9)
```

### Sentinel 2

```{python}
confusionMatrixGTB_sen = (testing_sen
  .classify(trainedGTB_sen)
  .errorMatrix('byte_property', "classification"))
#convert to pandas dataframe with class info
confusion_sen = (pd.DataFrame(
  confusionMatrixGTB_sen.getInfo(),
  index=[class_values],
  columns =[class_values]
  ))
print('GTB Confusion Matrix for Sentinel 2:')
print(confusion_sen)

#reformat and save
confusion_sen['mission'] = 'Sentinel 2'
confusion_sen.reset_index(inplace = True)
confusion_sen = confusion_sen.rename(columns = {'level_0': 'class'})  
confusion_sen.to_csv('data/output/GTB_'+v_date+'_Sen2_confusion.csv', index = False)

acc_values_GTB_sen = (confusionMatrixGTB_sen.accuracy().getInfo())
print("GTB Confusion Overall Accuracy for Sentinel 2: ", acc_values_GTB_sen)
k_GTB_sen = (confusionMatrixGTB_sen.kappa().getInfo())
print("GTB kappa for S2: ", k_GTB_ls5)
fs_GTB_sen = (confusionMatrixGTB_sen.fscore().getInfo())
print('GTB fScore for each class: ', fs_GTB_sen)

```

### Collate model stats, save to data folder

First, we'll copy over some values and make a big pandas dataframe. Note that the df.copy() function unlinks the original list from the new one. Silly python.

```{python}
accuracy_heads = class_values.copy()
accuracy_heads.extend(['GTB_accuracy', 'GTB_kappa'])
landsat5_perf = fs_GTB_ls5.copy()
landsat5_perf.extend([acc_values_GTB_ls5, k_GTB_ls5])
landsat7_perf = fs_GTB_ls7.copy()
landsat7_perf.extend([acc_values_GTB_ls7, k_GTB_ls7])
landsat8_perf = fs_GTB_ls8.copy()
landsat8_perf.extend([acc_values_GTB_ls8, k_GTB_ls8])
landsat9_perf = fs_GTB_ls9.copy()
landsat9_perf.extend([acc_values_GTB_ls9, k_GTB_ls9])
sentinel2_perf = fs_GTB_sen.copy()
sentinel2_perf.extend([acc_values_GTB_sen, k_GTB_sen])

performance_collation = pd.DataFrame(
  [landsat5_perf,
  landsat7_perf,
  landsat8_perf,
  landsat9_perf,
  sentinel2_perf],
  index = [
    'Landsat 5',
    'Landsat 7',
    'Landsat 8', 
    'Landsat 9',
    'Sentinel 2'
    ],
  columns = [accuracy_heads]
  )

# reset the index
performance_collation.reset_index(inplace = True)
performance_collation.rename(columns = {'index':'satellite'}).to_csv('data/output/GTB_'+v_date+'_performance_stats.csv', index = False)
```

## Apply model to image stack for Landsat

We'll go through and apply the Landsat and Sentinel separately due to the
differences in bands.

### Load the image collection

```{python}
# list bandnames; in part for L7 bands to match l8/9
bn457 = ['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7', 'QA_RADSAT']
bn89 = ['SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7', 'QA_RADSAT']
bn = ['Blue', 'Green', 'Red', 'B5', 'B6', 'B7', 'QA_RADSAT']

# filter stack for desired PRs
ROWS = ee.List([27, 28])

def applyScaleFactors(image):
  opticalBands = image.select(bn).multiply(0.0000275).add(-0.2) #flagging this - this applies the offset to QA RADSAT. :(
  return image.addBands(opticalBands, None, True)

# load Landsat 5, 7, 8, 9 Surface Reflectance and rename bands
l9 = (ee.ImageCollection("LANDSAT/LC09/C02/T1_L2")
  .filter(ee.Filter.eq('WRS_PATH', 26))
  .filter(ee.Filter.inList('WRS_ROW', ROWS))
  .filter(ee.Filter.gte('IMAGE_QUALITY_OLI', 7))
  .select(bn89, bn)
  .map(applyScaleFactors))
l8 = (ee.ImageCollection("LANDSAT/LC08/C02/T1_L2")
  .filter(ee.Filter.eq('WRS_PATH', 26))
  .filter(ee.Filter.inList('WRS_ROW', ROWS))
  .filter(ee.Filter.gte('IMAGE_QUALITY_OLI', 7))
  .select(bn89, bn)
  .map(applyScaleFactors))
l7 = (ee.ImageCollection('LANDSAT/LE07/C02/T1_L2')
  .filter(ee.Filter.eq('WRS_PATH', 26))
  .filter(ee.Filter.inList('WRS_ROW', ROWS))
  .filter(ee.Filter.gte('IMAGE_QUALITY', 7))
  .select(bn457, bn)
  .map(applyScaleFactors))
l5 = (ee.ImageCollection('LANDSAT/LT05/C02/T1_L2')
  .filter(ee.Filter.eq('WRS_PATH', 26))
  .filter(ee.Filter.inList('WRS_ROW', ROWS))
  .filter(ee.Filter.gte('IMAGE_QUALITY', 7))
  .select(bn457, bn)
  .map(applyScaleFactors))
```

Add QA_RADSAT mask to mask where any band was saturated [[adding flag here!!]]

```{python}
def apply_radsat_mask(image):
  radsat = image.select('QA_RADSAT').eq(0)
  return image.updateMask(radsat)

l9 = l9.map(apply_radsat_mask)
l8 = l8.map(apply_radsat_mask)
l7 = l7.map(apply_radsat_mask)
l5 = l5.map(apply_radsat_mask)
```

### Load modeling AOIs and clip stack

Note, some AOIs are too big to load in here and use as 'virtual' ee Feature
Collections. Given that, we have manually uploaded the shapefiles as an Earth Engine Feature Collection.

```{python}
# aoi_gpd = gpd.read_file('data/aoi/Superior_AOI_modeling.shp')
# aoi_ee = gm.geopandas_to_ee(aoi_gpd)

aoi_ee = ee.FeatureCollection('projects/ee-ross-superior/assets/aoi/Superior_AOI_modeling')

#Calculate total area of AOI
def calc_area(feat):
  feat_area = feat.geometry().area()
  feat_area_ha = ee.Number(feat_area).divide(1e5)
  return feat.set('area_ha', feat_area_ha)

aoi_area = aoi_ee.map(calc_area).aggregate_sum('area_ha')
aoi_area_ha = aoi_area.getInfo()
print('total AOI area: ', aoi_area_ha)
```

And then clip each image by that aoi

```{python}
# clip images to aoi
def clip(image):
  return image.clip(aoi_ee.geometry())

l5_aoi = l5.map(clip)
l7_aoi = l7.map(clip)
l8_aoi = l8.map(clip)
l9_aoi = l9.map(clip)
```

Load other AOIs to summarize over

```{python}
# aoi_no_sc_gpd = gpd.read_file('data/aoi/Superior_AOI_minus_shoreline_contamination.shp')
# aoi_no_sc_ee = gm.geopandas_to_ee(aoi_no_sc_gpd)
aoi_no_sc_ee = ee.FeatureCollection('projects/ee-ross-superior/assets/aoi/Superior_AOI_minus_shoreline_contamination')

aoi_no_sc_area = aoi_no_sc_ee.map(calc_area).aggregate_sum('area_ha')
aoi_no_sc_area_ha = aoi_no_sc_area.getInfo()
print('total AOI area without shoreline contamination: ', aoi_no_sc_area_ha)

# aoi_sc_gpd = gpd.read_file('data/aoi/Superior_shoreline_contamination.shp')
# aoi_sc_ee = gm.geopandas_to_ee(aoi_sc_gpd)
aoi_sc_ee = ee.FeatureCollection('projects/ee-ross-superior/assets/aoi/Superior_shoreline_contamination')

aoi_sc_area = aoi_sc_ee.map(calc_area).aggregate_sum('area_ha')
aoi_sc_area_ha = aoi_sc_area.getInfo()
print('total AOI area identified as shoreline contamination: ', aoi_sc_area_ha)
```

#### Helper functions

```{python}
# get CRS info
img_crs = l5.first().projection()
img_crsTrans = img_crs.getInfo().get('transform')

#function to apply the GTB model
def applyGTB_ls5(image):
  # Select the bands that correspond to the input features of the GTB model
  imageFeatures = image.select(ls_input_feat)
  missDate = image.get('missDate')
  # Classify the image using the trained GTB model
  classifiedImage = (imageFeatures
    .classify(trainedGTB_ls5)
    .set('missDate', missDate))
  return image.addBands(classifiedImage)

def applyGTB_ls7(image):
  # Select the bands that correspond to the input features of the GTB model
  imageFeatures = image.select(ls_input_feat)
  missDate = image.get('missDate')
  # Classify the image using the trained GTB model
  classifiedImage = (imageFeatures
    .classify(trainedGTB_ls7)
    .set('missDate', missDate))
  return image.addBands(classifiedImage)

def applyGTB_ls8(image):
  # Select the bands that correspond to the input features of the GTB model
  imageFeatures = image.select(ls_input_feat)
  missDate = image.get('missDate')
  # Classify the image using the trained GTB model
  classifiedImage = (imageFeatures
    .classify(trainedGTB_ls8)
    .set('missDate', missDate))
  return image.addBands(classifiedImage)

def applyGTB_ls9(image):
  # Select the bands that correspond to the input features of the GTB model
  imageFeatures = image.select(ls_input_feat)
  missDate = image.get('missDate')
  # Classify the image using the trained GTB model
  classifiedImage = (imageFeatures
    .classify(trainedGTB_ls9)
    .set('missDate', missDate))
  return image.addBands(classifiedImage)


# save each value as its own band and mask
def extract_classes(image):
  cl = image.select('classification')
  cloud = cl.eq(0).rename('cloud').selfMask()
  openWater = cl.eq(1).rename('openWater').selfMask()
  lightNSSed = cl.eq(2).rename('lightNSSed').selfMask()
  OSSed = cl.eq(3).rename('OSSed').selfMask()
  dNSSed = cl.eq(4).rename('dNSSed').selfMask()
  classified = cl.gte(0).rename('classified').selfMask()
  img_addBand = (image.addBands(cloud)
    .addBands(openWater)
    .addBands(lightNSSed)
    .addBands(OSSed)
    .addBands(dNSSed)
    .addBands(classified))
  return img_addBand


def applyPerMissionDate_ls5(missDate):
  mission = ee.String(missDate).slice(0,9)
  date = ee.String(missDate).slice(10,20)
  short_stack = (l5
    .filter(ee.Filter.eq('SPACECRAFT_ID', mission))
    .filter(ee.Filter.eq('DATE_ACQUIRED', date)))
  oneMissDate = short_stack.mean()
  ls_miss_date_GTB = applyGTB_ls5(oneMissDate)
  ls_GTB_class = extract_classes(ls_miss_date_GTB)
  return (ls_GTB_class.set('missDate', missDate))

def applyPerMissionDate_ls7(missDate):
  mission = ee.String(missDate).slice(0,9)
  date = ee.String(missDate).slice(10,20)
  short_stack = (l7
    .filter(ee.Filter.eq('SPACECRAFT_ID', mission))
    .filter(ee.Filter.eq('DATE_ACQUIRED', date)))
  oneMissDate = short_stack.mean()
  ls_miss_date_GTB = applyGTB_ls7(oneMissDate)
  ls_GTB_class = extract_classes(ls_miss_date_GTB)
  return (ls_GTB_class.set('missDate', missDate))

def applyPerMissionDate_ls8(missDate):
  mission = ee.String(missDate).slice(0,9)
  date = ee.String(missDate).slice(10,20)
  short_stack = (l8
    .filter(ee.Filter.eq('SPACECRAFT_ID', mission))
    .filter(ee.Filter.eq('DATE_ACQUIRED', date)))
  oneMissDate = short_stack.mean()
  ls_miss_date_GTB = applyGTB_ls8(oneMissDate)
  ls_GTB_class = extract_classes(ls_miss_date_GTB)
  return (ls_GTB_class.set('missDate', missDate))

def applyPerMissionDate_ls9(missDate):
  mission = ee.String(missDate).slice(0,9)
  date = ee.String(missDate).slice(10,20)
  short_stack = (l9
    .filter(ee.Filter.eq('SPACECRAFT_ID', mission))
    .filter(ee.Filter.eq('DATE_ACQUIRED', date)))
  oneMissDate = short_stack.mean()
  ls_miss_date_GTB = applyGTB_ls9(oneMissDate)
  ls_GTB_class = extract_classes(ls_miss_date_GTB)
  return (ls_GTB_class.set('missDate', missDate))

```

### consolidate stack by image date

```{python}
def addImageDate(image):
  mission = image.get('SPACECRAFT_ID')
  date = image.date().format('YYYY-MM-dd')
  missDate = ee.String(mission).cat('_').cat(ee.String(date))
  return image.set('missDate', missDate)

l5_aoi = l5_aoi.map(addImageDate)
l7_aoi = l7_aoi.map(addImageDate)
l8_aoi = l8_aoi.map(addImageDate)
l9_aoi = l9_aoi.map(addImageDate)

# summarize by missionDate field
uniqueMissDate_l5 = l5_aoi.aggregate_array('missDate').distinct()
uniqueMissDate_l7 = l7_aoi.aggregate_array('missDate').distinct()
uniqueMissDate_l8 = l8_aoi.aggregate_array('missDate').distinct()
uniqueMissDate_l9 = l9_aoi.aggregate_array('missDate').distinct()

```

### Create mosaics

```{python}
def mosaicStack_l5(missDate):
  md_GTB = applyPerMissionDate_ls5(missDate)
  return md_GTB

def mosaicStack_l7(missDate):
  md_GTB = applyPerMissionDate_ls7(missDate)
  return md_GTB

def mosaicStack_l8(missDate):
  md_GTB = applyPerMissionDate_ls8(missDate)
  return md_GTB

def mosaicStack_l9(missDate):
  md_GTB = applyPerMissionDate_ls9(missDate)
  return md_GTB

newStack_list_l5 = uniqueMissDate_l5.map(mosaicStack_l5)
newStack_l5 = ee.ImageCollection(newStack_list_l5)

newStack_list_l7 = uniqueMissDate_l7.map(mosaicStack_l7)
newStack_l7 = ee.ImageCollection(newStack_list_l7)

newStack_list_l8 = uniqueMissDate_l8.map(mosaicStack_l8)
newStack_l8 = ee.ImageCollection(newStack_list_l8)

newStack_list_l9 = uniqueMissDate_l9.map(mosaicStack_l9)
newStack_l9 = ee.ImageCollection(newStack_list_l9)

```

### Lighten up each of the stacks to only the bands we care about

```{python}
lightStack_l5 = newStack_l5.select([
    'classified',
    'cloud',
    'openWater',
    'lightNSSed',
    'OSSed',
    'dNSSed'
    ])

lightStack_l7 = newStack_l7.select([
    'classified',
    'cloud',
    'openWater',
    'lightNSSed',
    'OSSed',
    'dNSSed'
    ])

lightStack_l8 = newStack_l8.select([
    'classified',
    'cloud',
    'openWater',
    'lightNSSed',
    'OSSed',
    'dNSSed'
    ])

lightStack_l9 = newStack_l9.select([
    'classified',
    'cloud',
    'openWater',
    'lightNSSed',
    'OSSed',
    'dNSSed'
    ])
```

### Calculate area per image and export

```{python}
#because these geometries are rather large, we have to create a function for each AOI
def calcArea_aoi(image):
  areaImage =  (image
    .multiply(ee.Image.pixelArea()).divide(1e5))

  area = areaImage.reduceRegions(
    collection = aoi_ee,
    reducer = ee.Reducer.sum().forEachBand(areaImage),
    crs = img_crs,
    crsTransform = img_crsTrans
  )

  missDate = image.get('missDate')

  # Create a feature with the calculated area and properties
  a = area.first().set({'missDate': missDate, 
                        'area_ha': aoi_area_ha, 
                        'extent': 'aoi'})
  
  return ee.FeatureCollection(a)


def calcArea_nosc(image):
  areaImage =  (image
    .multiply(ee.Image.pixelArea()).divide(1e5)
    .clip(aoi_no_sc_ee.geometry()))
  
  area = areaImage.reduceRegions(
    collection = aoi_no_sc_ee,
    reducer = ee.Reducer.sum().forEachBand(areaImage),
    crs = img_crs,
    crsTransform = img_crsTrans
  )
  
  missDate = image.get('missDate')
  
  # Create a feature with the calculated area and properties
  a = area.first().set({'missDate': missDate, 
                        'area_ha': aoi_no_sc_area_ha, 
                        'extent': 'aoi no sc'})
  
  return ee.FeatureCollection(a)


def calcArea_sc(image):
  areaImage =  (image
    .multiply(ee.Image.pixelArea()).divide(1e5)
    .clip(aoi_sc_ee.geometry()))

  area = areaImage.reduceRegions(
    collection = aoi_sc_ee,
    reducer = ee.Reducer.sum().forEachBand(areaImage),
    crs = img_crs,
    crsTransform = img_crsTrans
  )

  missDate = image.get('missDate')

  # Create a feature with the calculated area and properties
  a = area.first().set({'missDate': missDate, 
                        'area_ha': aoi_sc_area_ha, 
                        'extent': 'shoreline contamination'})
  
  return ee.FeatureCollection(a)

```

And then map them over all the stacks

```{python}

allAreas_l5_aoi = lightStack_l5.map(calcArea_aoi)
allAreas_l7_aoi = lightStack_l7.map(calcArea_aoi)
allAreas_l8_aoi = lightStack_l8.map(calcArea_aoi)
allAreas_l9_aoi = lightStack_l9.map(calcArea_aoi)

allAreas_l5_nosc = lightStack_l5.map(calcArea_nosc)
allAreas_l7_nosc = lightStack_l7.map(calcArea_nosc)
allAreas_l8_nosc = lightStack_l8.map(calcArea_nosc)
allAreas_l9_nosc = lightStack_l9.map(calcArea_nosc)

allAreas_l5_sc = lightStack_l5.map(calcArea_sc)
allAreas_l7_sc = lightStack_l7.map(calcArea_sc)
allAreas_l8_sc = lightStack_l8.map(calcArea_sc)
allAreas_l9_sc = lightStack_l9.map(calcArea_sc)

```

And then, sadly, these individual aoi geometries are too big to merge, so you
have to export them each individually

#### Landsat 5

```{python}
## export to drive
export_summary_5_aoi = (ee.batch.Export.table.toDrive(
  collection = allAreas_l5_aoi,
  selectors = ['missDate', 'classified', 'cloud', 'openWater', 'lightNSSed','OSSed', 'dNSSed', 'area_ha', 'extent'],
  description = 'gradientTreeBoost_landsat_5_stack_v' + v_date,
  folder = 'eePlumB_classification',
  fileFormat = 'csv'))


#export_summary_5_aoi.start()

export_summary_5_nosc = (ee.batch.Export.table.toDrive(
  collection = allAreas_l5_nosc,
  selectors = ['missDate', 'classified', 'cloud', 'openWater', 'lightNSSed','OSSed', 'dNSSed', 'area_ha', 'extent'],
  description = 'gradientTreeBoost_landsat_5_stack_nosc_v' + v_date,
  folder = 'eePlumB_classification',
  fileFormat = 'csv'))

#export_summary_5_nosc.start()

export_summary_5_sc = (ee.batch.Export.table.toDrive(
  collection = allAreas_l5_sc,
  selectors = ['missDate', 'classified', 'cloud', 'openWater', 'lightNSSed','OSSed', 'dNSSed', 'area_ha', 'extent'],
  description = 'gradientTreeBoost_landsat_5_stack_sc_v' + v_date,
  folder = 'eePlumB_classification',
  fileFormat = 'csv'))

#export_summary_5_sc.start()
```

#### Landsat 7

```{python}
export_summary_7_aoi = (ee.batch.Export.table.toDrive(
  collection = allAreas_l7_aoi,
  selectors = ['missDate', 'classified', 'cloud', 'openWater', 'lightNSSed','OSSed', 'dNSSed', 'area_ha', 'extent'],
  description = 'gradientTreeBoost_landsat_7_stack_v' + v_date,
  folder = 'eePlumB_classification',
  fileFormat = 'csv'))

#export_summary_7_aoi.start()

export_summary_7_nosc = (ee.batch.Export.table.toDrive(
  collection = allAreas_l7_nosc,
  selectors = ['missDate', 'classified', 'cloud', 'openWater', 'lightNSSed','OSSed', 'dNSSed', 'area_ha', 'extent'],
  description = 'gradientTreeBoost_landsat_7_stack_nosc_v' + v_date,
  folder = 'eePlumB_classification',
  fileFormat = 'csv'))

#export_summary_7_nosc.start()

export_summary_7_sc = (ee.batch.Export.table.toDrive(
  collection = allAreas_l7_sc,
  selectors = ['missDate', 'classified', 'cloud', 'openWater', 'lightNSSed','OSSed', 'dNSSed', 'area_ha', 'extent'],
  description = 'gradientTreeBoost_landsat_7_stack_sc_v' + v_date,
  folder = 'eePlumB_classification',
  fileFormat = 'csv'))

#export_summary_7_sc.start()
```

#### Landsat 8

```{python}
export_summary_8_aoi = (ee.batch.Export.table.toDrive(
  collection = allAreas_l8_aoi,
  selectors = ['missDate', 'classified', 'cloud', 'openWater', 'lightNSSed','OSSed', 'dNSSed', 'area_ha', 'extent'],
  description = 'gradientTreeBoost_landsat_8_stack_v' + v_date,
  folder = 'eePlumB_classification',
  fileFormat = 'csv'))

#export_summary_8_aoi.start()

export_summary_8_nosc = (ee.batch.Export.table.toDrive(
  collection = allAreas_l8_nosc,
  selectors = ['missDate', 'classified', 'cloud', 'openWater', 'lightNSSed','OSSed', 'dNSSed', 'area_ha', 'extent'],
  description = 'gradientTreeBoost_landsat_8_stack_nosc_v' + v_date,
  folder = 'eePlumB_classification',
  fileFormat = 'csv'))

#export_summary_8_nosc.start()

export_summary_8_sc = (ee.batch.Export.table.toDrive(
  collection = allAreas_l8_sc,
  selectors = ['missDate', 'classified', 'cloud', 'openWater', 'lightNSSed','OSSed', 'dNSSed', 'area_ha', 'extent'],
  description = 'gradientTreeBoost_landsat_8_stack_sc_v' + v_date,
  folder = 'eePlumB_classification',
  fileFormat = 'csv'))

#export_summary_8_sc.start()
```

#### Landsat 9

```{python}
export_summary_9_aoi = (ee.batch.Export.table.toDrive(
  collection = allAreas_l9_aoi,
  selectors = ['missDate', 'classified', 'cloud', 'openWater', 'lightNSSed','OSSed', 'dNSSed', 'area_ha', 'extent'],
  description = 'gradientTreeBoost_landsat_9_stack_v' + v_date,
  folder = 'eePlumB_classification',
  fileFormat = 'csv'))

#export_summary_9_aoi.start()

export_summary_9_nosc = (ee.batch.Export.table.toDrive(
  collection = allAreas_l9_nosc,
  selectors = ['missDate', 'classified', 'cloud', 'openWater', 'lightNSSed','OSSed', 'dNSSed', 'area_ha', 'extent'],
  description = 'gradientTreeBoost_landsat_9_stack_nosc_v' + v_date,
  folder = 'eePlumB_classification',
  fileFormat = 'csv'))

#export_summary_9_nosc.start()

export_summary_9_sc = (ee.batch.Export.table.toDrive(
  collection = allAreas_l9_sc,
  selectors = ['missDate', 'classified', 'cloud', 'openWater', 'lightNSSed','OSSed', 'dNSSed', 'area_ha', 'extent'],
  description = 'gradientTreeBoost_landsat_9_stack_sc_v' + v_date,
  folder = 'eePlumB_classification',
  fileFormat = 'csv'))

#export_summary_9_sc.start()
```

## Apply model to image stack for Sentinel

### Load the image collection

```{python}
# list bandnames; in part for L7 bands to match l8/9
bnsen = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B11', 'B12']
bn = ['Blue', 'Green', 'Red', 'B5', 'B6', 'B7', 'B8', 'B11', 'B12']

# filter stack for desired tiles
TILES = ee.List(['15TWN', '15TXN', '15TYN', '15TWM', '15TXM', '15TYM'])

s2 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')

# apply scaling factor
def applySenScale(image):
  optical = image.select('B.').multiply(0.0001)
  optical2 = image.select('B..').multiply(0.0001)
  return image.addBands(optical, None, True).addBands(optical2, None, True)

sen = (s2
  .filter(ee.Filter.inList('MGRS_TILE', TILES))
  .map(applySenScale)
  .select(bnsen, bn))

resample_crs_transform = sen.first().select('Red').projection().getInfo().get('transform')

targetCRS = 'EPSG:32615'  # UTM Zone 15N
targetScale = 10  # 10m resolution

# and now resample the 20m pixels to 10m
def reprojectBands(image):
  bands = ['B5', 'B6', 'B7', 'B11', 'B12']
  reprojectedBands = [image.select(band).resample('bilinear').reproject(
    crs = targetCRS,
    scale = targetScale
  ).rename(band) for band in bands]
  return image.addBands(reprojectedBands, None, True)

# Apply the resample function to the image collection
sen = sen.map(reprojectBands)

```

And then clip each image by the aoi

```{python}
sen_aoi = sen.map(clip)

# get CRS info
img_crs_sen = sen_aoi.first().select('Red').projection()
img_crsTrans_sen = img_crs_sen.getInfo().get('transform')

```

#### Helper functions

```{python}
#function to apply the GTB model
def applyGTB_sen(image):
  # Select the bands that correspond to the input features of the GTB model
  imageFeatures = image.select(sen_input_feat)
  missDate = image.get('missDate')
  # Classify the image using the trained GTB model
  classifiedImage = (imageFeatures
    .classify(trainedGTB_sen)
    .set('missDate', missDate))
  return image.addBands(classifiedImage)

def applyPerMissionDate_sen(missDate):
  mission = ee.String(missDate).slice(0,11)
  date = ee.String(missDate).slice(12,22)
  short_stack = (sen
    .filter(ee.Filter.eq('SPACECRAFT_NAME', mission))
    .filterDate(date, ee.Date(date).advance(1,'day')))
  oneMissDate = short_stack.mean()
  sen_miss_date_GTB = applyGTB_sen(oneMissDate)
  sen_GTB_class = extract_classes(sen_miss_date_GTB)
  return (sen_GTB_class.set('missDate', missDate))

```

### Consolidate stack by image date

```{python}
def addImageDate(image):
  mission = image.get('SPACECRAFT_NAME')
  date = image.date().format('YYYY-MM-dd')
  missDate = ee.String(mission).cat('_').cat(ee.String(date))
  return image.set('missDate', missDate)

sen_aoi = sen_aoi.map(addImageDate)

# summarize by missionDate field
uniqueMissDate_sen = sen_aoi.aggregate_array('missDate').distinct()

```

And then mosaic per mission date.

```{python}
def mosaicStack_sen(missDate):
  md_GTB = applyPerMissionDate_sen(missDate)
  return md_GTB

newStack_list_sen = uniqueMissDate_sen.map(mosaicStack_sen)
newStack_sen = ee.ImageCollection(newStack_list_sen)
```

### Calculate area per image and export

```{python}
sen_stack_light = newStack_sen.select([
    'classified',
    'cloud',
    'openWater',
    'lightNSSed',
    'OSSed',
    'dNSSed'
    ])

def calcArea_sen_aoi(image):
  areaImage =  image.multiply(ee.Image.pixelArea()).divide(1e5)
  
  area = areaImage.reduceRegions(
    collection = aoi_ee,
    reducer = ee.Reducer.sum().forEachBand(areaImage),
    crs = img_crs_sen,
    crsTransform = img_crsTrans_sen
  )
  
  missDate = image.get('missDate')
  
  # Create a feature with the calculated area and properties
  a = area.first().set({'missDate': missDate, 
                        'area_ha': aoi_area_ha,
                        'extent': 'aoi'})
  
  return ee.FeatureCollection(a)


allAreas_sen_aoi = sen_stack_light.map(calcArea_sen_aoi)

## export to drive
export_summary_sen_aoi = (ee.batch.Export.table.toDrive(
  collection = allAreas_sen_aoi,
  selectors = ['missDate', 'classified', 'cloud', 'openWater', 'lightNSSed','OSSed', 'dNSSed', 'area_ha', 'aoi'],
  description = 'gradientTreeBoost_sen2_stack_v' + v_date,
  folder = 'eePlumB_classification',
  fileFormat = 'csv'))

#export_summary_sen_aoi.start()

def calcArea_sen_nosc(image):
  areaImage =  image.multiply(ee.Image.pixelArea()).divide(1e5)
  
  area = areaImage.reduceRegions(
    collection = aoi_no_sc_ee,
    reducer = ee.Reducer.sum().forEachBand(areaImage),
    crs = img_crs_sen,
    crsTransform = img_crsTrans_sen
  )
  
  missDate = image.get('missDate')
  
  # Create a feature with the calculated area and properties
  a = area.first().set({'missDate': missDate, 
                        'area_ha': aoi_no_sc_area_ha,
                        'extent': 'aoi no sc'})
  
  return ee.FeatureCollection(a)


allAreas_sen_nosc = sen_stack_light.map(calcArea_sen_nosc)

## export to drive
export_summary_sen_nosc = (ee.batch.Export.table.toDrive(
  collection = allAreas_sen_nosc,
  selectors = ['missDate', 'classified', 'cloud', 'openWater', 'lightNSSed','OSSed', 'dNSSed', 'area_ha', 'extent'],
  description = 'gradientTreeBoost_sen2_nosc_stack_v' + v_date,
  folder = 'eePlumB_classification',
  fileFormat = 'csv'))

#export_summary_sen_nosc.start()



def calcArea_sen_sc(image):
  areaImage =  image.multiply(ee.Image.pixelArea()).divide(1e5)
  
  area = areaImage.reduceRegions(
    collection = aoi_sc_ee,
    reducer = ee.Reducer.sum().forEachBand(areaImage),
    crs = img_crs_sen,
    crsTransform = img_crsTrans_sen
  )
  
  missDate = image.get('missDate')
  
  # Create a feature with the calculated area and properties
  a = area.first().set({'missDate': missDate, 
                        'area_ha': aoi_sc_area_ha,
                        'extent': 'shoreline contamination'})
  
  return ee.FeatureCollection(a)


allAreas_sen_sc = sen_stack_light.map(calcArea_sen_sc)

## export to drive
export_summary_sen_sc = (ee.batch.Export.table.toDrive(
  collection = allAreas_sen_sc,
  selectors = ['missDate', 'classified', 'cloud', 'openWater', 'lightNSSed','OSSed', 'dNSSed', 'area_ha', 'extent'],
  description = 'gradientTreeBoost_sen2_sc_stack_v' + v_date,
  folder = 'eePlumB_classification',
  fileFormat = 'csv'))

#export_summary_sen_sc.start()

```


## Export GeoTiffs to drive

Load task checker!

```{python}
##Function for limiting the max number of tasks sent to
#earth engine at one time to avoid time out errors
def maximum_no_of_tasks(MaxNActive, waitingPeriod):
  ##maintain a maximum number of active tasks
  ## initialize submitting jobs
  ts = list(ee.batch.Task.list())
  NActive = 0
  for task in ts:
     if ('RUNNING' in str(task) or 'READY' in str(task)):
         NActive += 1
  ## wait if the number of current active tasks reach the maximum number
  ## defined in MaxNActive
  while (NActive >= MaxNActive):
    time.sleep(waitingPeriod) # if reach or over maximum no. of active tasks, wait for 2min and check again
    ts = list(ee.batch.Task.list())
    NActive = 0
    for task in ts:
      if ('RUNNING' in str(task) or 'READY' in str(task)):
        NActive += 1
  return()

```

### Raster Export

### GTB images for Landsat 5

```{python}
date_length_5 = len(uniqueMissDate_l5.getInfo())

for d in range(date_length_5):
  md = uniqueMissDate_l5.get(d)
  image = (newStack_l5
    .filter(ee.Filter.eq('missDate', md))
    .first()
    .select('classification'))
  export_image = ee.batch.Export.image.toDrive(
    image = image,
    region = aoi_ee.geometry(),
    description = 'GTB_v' + v_date + '_' + str(md.getInfo()),
    folder = 'GTB_v'+v_date,
    scale = 30,
    crs = img_crs,
    maxPixels = 1e13)
  #Check how many existing tasks are running and take a break of 30 mins if it's >25
  maximum_no_of_tasks(10, 5*60)
  #Send next task.
  export_image.start()

```

### GTB images for Landsat 7

```{python}
date_length_7 = len(uniqueMissDate_l7.getInfo())

for d in range(date_length_7):
  md = uniqueMissDate_l7.get(d)
  image = (newStack_l7
    .filter(ee.Filter.eq('missDate', md))
    .first()
    .select('classification'))
  export_image = ee.batch.Export.image.toDrive(
    image = image,
    region = aoi_ee.geometry(),
    description = 'GTB_v' + v_date + '_' + str(md.getInfo()),
    folder = 'GTB_v'+v_date,
    scale = 30,
    crs = img_crs,
    maxPixels = 1e13)
  #Check how many existing tasks are running and take a break of 30 mins if it's >25
  maximum_no_of_tasks(10, 5*60)
  #Send next task.
  export_image.start()

```


### GTB images for Landsat 8

```{python}
date_length_8 = len(uniqueMissDate_l8.getInfo())

for d in range(date_length_8):
  md = uniqueMissDate_l8.get(d)
  image = (newStack_l8
    .filter(ee.Filter.eq('missDate', md))
    .first()
    .select('classification'))
  export_image = ee.batch.Export.image.toDrive(
    image = image,
    region = aoi_ee.geometry(),
    description = 'GTB_v' + v_date + '_' + str(md.getInfo()),
    folder = 'GTB_v'+v_date,
    scale = 30,
    crs = img_crs,
    maxPixels = 1e13)
  #Check how many existing tasks are running and take a break of 30 mins if it's >25
  maximum_no_of_tasks(10, 5*60)
  #Send next task.
  export_image.start()

```


### GTB images for Landsat 9

```{python}
date_length_9 = len(uniqueMissDate_l9.getInfo())

for d in range(date_length_9):
  md = uniqueMissDate_l9.get(d)
  image = (newStack_l9
    .filter(ee.Filter.eq('missDate', md))
    .first()
    .select('classification'))
  export_image = ee.batch.Export.image.toDrive(
    image = image,
    region = aoi_ee.geometry(),
    description = 'GTB_v' + v_date + '_' + str(md.getInfo()),
    folder = 'GTB_v'+v_date,
    scale = 30,
    crs = img_crs,
    maxPixels = 1e13)
  #Check how many existing tasks are running and take a break of 30 mins if it's >25
  maximum_no_of_tasks(10, 5*60)
  #Send next task.
  export_image.start()

```

### GTB images for Sentinel 2

```{python}
date_length_sen = len(uniqueMissDate_sen.getInfo())

for d in range(date_length_sen):
  md = uniqueMissDate_sen.get(d)
  image = (newStack_sen
    .filter(ee.Filter.eq('missDate', md))
    .first()
    .select('classification'))
  export_image = ee.batch.Export.image.toDrive(
    image = image,
    region = aoi_ee.geometry(),
    description = 'GTB_v' + v_date + '_' + str(md.getInfo()),
    folder = 'GTB_v'+v_date,
    scale = 10,
    crs = img_crs_sen,
    maxPixels = 1e13)
  #Check how many existing tasks are running and take a break of 30 mins if it's >25
  maximum_no_of_tasks(10, 5*60)
  #Send next task.
  export_image.start()

```