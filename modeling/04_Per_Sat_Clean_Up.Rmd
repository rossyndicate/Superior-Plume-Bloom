---
title: "Yugo Model Clean Up"
author: "ROSSyndicate"
date: "2023-07-11"
output: html_document
---

```{r setup, echo = F}
libs = c('googledrive', 'tidyverse', 'readr')

package_loader <- function(x) {
    if (x %in% installed.packages()) {
      library(x, character.only = TRUE)
    } else {
      install.packages(x)
      library(x, character.only = TRUE)
    }
}

lapply(libs, package_loader)

v_date = '2023-07-20'
```

# Purpose

This script pulls down the tabular data export from GEE that lives in the
ROSSyndicate account and uploads to the Superior Folder after removing
burndensome columns (thanks .geo) and makes some calculations.

```{r, echo = F}
drive_auth() # authenticate for ROSSyndicate account
```

## Grab and clean up files

```{r}
drive = drive_ls(recursive = T)
landsat_GTB = (drive[grepl(v_date, drive$name),])
print(landsat_GTB)
```

```{r}
drive_downloader = function(driveid, drivename) {
  drive_download(as_id(driveid), path = file.path('data/output/', drivename), overwrite = T)
}
```


```{r}
# if the labels directory exists
if (dir.exists('data/output/')) {
  # delete it and its contents
  unlink('data/output/', recursive = T)
  # then recreate
  dir.create('data/output/')
  # and download the labels file locally
  walk2(landsat_GTB$id, landsat_GTB$name, drive_downloader)
} else {
  dir.create('data/output/')
  walk2(landsat_GTB$id, landsat_GTB$name, drive_downloader)
}
```

Read in files

```{r}
read_in_files <- function(filename) {
  read_csv(file.path('data/output/', filename))
}

rs <- map_dfr(landsat_GTB$name, read_in_files)

rs_subset <- rs %>%
  select(missDate, 
         #area_ha,
         classified_ha = classified,
         openWater_ha = openWater,
         cloud_ha = cloud,
         lightNSSed_ha = lightNSSed,
         darkNSSed_ha = dNSSed,
         offShoreSed_ha = OSSed)
```

Calculate proportions - looks like there is a slight mismatch in the area
calculation (difference between a polygon size and a raster size), so we'll
calculate the proportion of total area for which we have classification data,
and then proportions of the classified area which are the particular classes.
Note that the proportion of total area may be greater than 100% due to that
measurement mis-match.

```{r}
area_ha = 77102.7869 #need to export this.

# calculate proportion of AOI that is classified
rs_subset <- rs_subset %>%
  mutate(proportion_AOI_classified = (classified_ha/area_ha)*100)

# print range of proportion of area classified
range(rs_subset$proportion_AOI_classified)

# calculate the proportion of classified area that are each class
rs_class_summ <- rs_subset %>%
  mutate_at(vars(openWater_ha, cloud_ha, lightNSSed_ha, darkNSSed_ha, offShoreSed_ha),
            ~ (./classified_ha)*100) %>%
  select(missDate, openWater_ha, cloud_ha, lightNSSed_ha, darkNSSed_ha, offShoreSed_ha) %>%
  rename_with(~ str_replace(., "_ha$", "_perc"), ends_with("_ha"))

# join them together
rs_subset <- full_join(rs_subset, rs_class_summ)
```

Reformat the mission date column

```{r}
rs_subset <- rs_subset %>%
  mutate(mission = str_sub(missDate, 1, 9),
         date = str_sub(missDate, 11, nchar(missDate))) %>%
  relocate(mission, date)
```

Write to tmp folder

```{r}
write_csv(rs_subset, 'data/output/GTB_by_LS_mission_first_pass.csv')
```

## Save to the Superior Drive

Re-authorize without the ROSS account
```{r}
drive_auth()
```

Find the folder

```{r}
drive_folder = drive_ls(path = 'Superior Plume-Bloom Project')
upload_to = drive_folder[grepl('classification', drive_folder$name),]

drive_upload('data/output/GTB_by_LS_mission_first_pass.csv', as_id(upload_to$id[1]))
```



```{r wrap, echo = F}
knitr::wrap_rmd('modeling/02_Yugo_Clean_Up.Rmd', width = 80, backup = NULL)
```
