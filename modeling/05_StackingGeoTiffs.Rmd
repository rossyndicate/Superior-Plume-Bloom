---
title: "Stacking GeoTiffs"
author: "ROSSyndicate"
date: "2023-08-10"
output: html_document
---

```{r}
packages <- c('tidyverse',
              'googledrive',
              'reticulate')

package_loader <- function(x) {
    if (x %in% installed.packages()) {
      library(x, character.only = TRUE)
    } else {
      install.packages(x)
      library(x, character.only = TRUE)
    }
}

lapply(packages, package_loader)

#authenticate google
drive_auth()
```

## Activate conda environment

Check for virtual environment and activate, otherwise, set up virtual environment.

```{r conda env}
if (!dir.exists("env")) {
  source("pySetup.R")
} else {
  use_condaenv(file.path(getwd(), "env"))
}
```

Load python modules.

```{python}
#load modules
import xarray as xr
import os
import rasterio
import rioxarray as rxr
from scipy import ndimage as ndi
```

# Purpose

This is a framework script to stack multiple geotiffs into netCDF files for the Superior Plume-Bloom project.

# Pre-work

## Download tifs from Drive

First, we'll create a temporary local folder

```{r}
dir.create('tmp')
dir.create('tmp/nc')
```

And then download the files from drive.

```{r}
#get the file info
tifs <- drive_ls(path = 'GTB_v2023-08-03')
```

And our helper function for download to the temp folder

```{r}
drive_downloader = function(driveid, drivename, dest) {
  drive_download(as_id(driveid), path = file.path('tmp', drivename), overwrite = T)
}
```

And then map across the files

```{r}
walk2(tifs$id, tifs$name, drive_downloader)
```

## Prep workspace

List files, and write the function to get a single year of data

```{python}
# list all files in the directory
files = os.listdir('tmp')

def correct_landsat_convention(file):
  return file.replace('SAT_', 'SAT-')

files_corrected = *map(correct_landsat_convention, files),

# function to get the individual years from file name
def getYear(file):
  f_list = file.split('_')
  date = f_list[3]
  year = date.split('-')[0]
  return year

#grab all years
years = *map(getYear, files_corrected),
#get unique years
years = set(years)
#save as list
years = list(years)
```

And now stack the files and save per year

```{python}
for y in enumerate(years):
  year = y[1]
  # create an empty xarray dataset to store the stacked data
  nc = xr.Dataset()
  #filter the list, but mak sure it only looks at the last chunk
  year_files =  [file for file in files if year in file[-14:]]

  # loop through the GeoTiffs and add them to the dataset
  for i, geotiff_file in enumerate(year_files):
    #get file info
    file_strings = correct_landsat_convention(geotiff_file).split('_')
    date = file_strings[3]
    date = date.split('.')[0]
    mission = file_strings[2]
    
    # open the GeoTiff with rasterio
    data = rxr.open_rasterio(os.path.join('tmp/', geotiff_file))
    transform = rasterio.open(os.path.join('tmp/', geotiff_file)).transform
    
    # create an xarray data array and add it to the dataset, define coords
    data_array = xr.DataArray(data,
      dims=('band', 'y', 'x'), 
      coords={'band': range(data.shape[0]), 'y': data.y, 'x': data.x})
    
    data_array.attrs['date'] = date
    data_array.attrs['mission'] = mission
    
    nc[date] = data_array
    
    #set file name
    fn = 'GTB_v2023-08-03_' + year + '_stack.nc'
    # save the dataset to a netCDF file
    nc.to_netcdf(os.path.join('tmp/nc/', fn))

```

# Save the netCDF to drive

```{r}
nc_folder = drive_ls(pattern = 'GTB_v2023-08-03_nc')

nc_files = list.files(dump_dir)
nc_files = 

uploadToDrive = function(file) {
  drive_upload(file.path(dump_dir, file),
               path = as_id(nc_folder$id),
               overwrite = T)
}

walk(nc_files, uploadToDrive)
```
